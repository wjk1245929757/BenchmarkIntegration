{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5e5e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding : utf-8-*-\n",
    "# coding:unicode_escape\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# the location of R (used for the mclust clustering)\n",
    "import os\n",
    "os.environ['R_HOME'] = \"D:\\\\anaconda\\envs\\STAligner\\Lib\\R\"\n",
    "os.environ['R_USER'] = \"D:\\\\anaconda\\envs\\STAligner\\Lib\\site-packages\\rpy2\"\n",
    "\n",
    "\n",
    "os.chdir(\"D:/bio/SPIRAL/SPIRAL-main\") \n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from spiral.main import SPIRAL_integration\n",
    "from spiral.layers import *\n",
    "from spiral.utils import *\n",
    "from spiral.CoordAlignment import CoordAlignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45de1579",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs=\"G:/dataset/07-seqFish-SpatialMouseAtlas/input/SPIRAL_input/\"\n",
    "sample_name=['embryo1-2', 'embryo1-5', 'embryo2-2','embryo2-5', 'embryo3-2', 'embryo3-5']\n",
    "for sample in sample_name:\n",
    "    df = pd.read_csv(dirs+sample+'_meta.csv', index_col=0)\n",
    "    df2 = df[['batch', 'celltype_mapped_refined']]\n",
    "    df2.to_csv(dirs+sample+'_label.csv')\n",
    "    df3 = df[['x_global', 'y_global']]\n",
    "    df3.to_csv(dirs+sample+'_coord.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "647bbff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs=\"G:/dataset/07-seqFish-SpatialMouseAtlas/input/SPIRAL_input/\"\n",
    "sample_name=['embryo1-2', 'embryo1-5', 'embryo2-2','embryo2-5', 'embryo3-2', 'embryo3-5']\n",
    "samples=sample_name\n",
    "SEP=','\n",
    "net_cate='_KNN_'\n",
    "rad=150\n",
    "knn=6\n",
    "\n",
    "N_WALKS=knn\n",
    "WALK_LEN=1\n",
    "N_WALK_LEN=knn\n",
    "NUM_NEG=knn\n",
    "\n",
    "feat_file=[]\n",
    "edge_file=[]\n",
    "meta_file=[]\n",
    "coord_file=[]\n",
    "flags=''\n",
    "flags1=str(samples[0])\n",
    "for i in range(1,len(samples)):\n",
    "    flags1=flags1+'-'+str(samples[i])\n",
    "for i in range(len(samples)):\n",
    "    feat_file.append(dirs+samples[i]+\"_features.csv\")\n",
    "    edge_file.append(dirs+samples[i]+\"_edge_KNN_\"+str(knn)+\".csv\")\n",
    "    meta_file.append(dirs+samples[i]+\"_label.csv\")\n",
    "    coord_file.append(dirs+samples[i]+\"_coord.csv\")\n",
    "    flags=flags+'_'+str(samples[i])\n",
    "                     \n",
    "N=pd.read_csv(feat_file[0],header=0,index_col=0).shape[1]\n",
    "if (len(samples)==2):\n",
    "    M=1\n",
    "else:\n",
    "    M=len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c3f2487",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=0, help='The seed of initialization.')\n",
    "parser.add_argument('--AEdims', type=list, default=[N,[512],32], help='Dim of encoder.')\n",
    "parser.add_argument('--AEdimsR', type=list, default=[32,[512],N], help='Dim of decoder.')\n",
    "parser.add_argument('--GSdims', type=list, default=[512,32], help='Dim of GraphSAGE.')\n",
    "parser.add_argument('--zdim', type=int, default=32, help='Dim of embedding.')\n",
    "parser.add_argument('--znoise_dim', type=int, default=4, help='Dim of noise embedding.')\n",
    "parser.add_argument('--CLdims', type=list, default=[4,[],M], help='Dim of classifier.')\n",
    "parser.add_argument('--DIdims', type=list, default=[28,[32,16],M], help='Dim of discriminator.')\n",
    "parser.add_argument('--beta', type=float, default=1.0, help='weight of GraphSAGE.')\n",
    "parser.add_argument('--agg_class', type=str, default=MeanAggregator, help='Function of aggregator.')\n",
    "parser.add_argument('--num_samples', type=str, default=knn, help='number of neighbors to sample.')\n",
    "\n",
    "parser.add_argument('--N_WALKS', type=int, default=N_WALKS, help='number of walks of random work for postive pairs.')\n",
    "parser.add_argument('--WALK_LEN', type=int, default=WALK_LEN, help='walk length of random work for postive pairs.')\n",
    "parser.add_argument('--N_WALK_LEN', type=int, default=N_WALK_LEN, help='number of walks of random work for negative pairs.')\n",
    "parser.add_argument('--NUM_NEG', type=int, default=NUM_NEG, help='number of negative pairs.')\n",
    "\n",
    "\n",
    "parser.add_argument('--epochs', type=int, default=100, help='Number of epochs to train.')\n",
    "parser.add_argument('--batch_size', type=int, default=1024, help='Size of batches to train.') ####512 for withon donor;1024 for across donor###\n",
    "parser.add_argument('--lr', type=float, default=1e-3, help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay.')\n",
    "parser.add_argument('--alpha1', type=float, default=N, help='Weight of decoder loss.')\n",
    "parser.add_argument('--alpha2', type=float, default=1, help='Weight of GraphSAGE loss.')\n",
    "parser.add_argument('--alpha3', type=float, default=1, help='Weight of classifier loss.')\n",
    "parser.add_argument('--alpha4', type=float, default=1, help='Weight of discriminator loss.')\n",
    "parser.add_argument('--lamda', type=float, default=1, help='Weight of GRL.')\n",
    "parser.add_argument('--Q', type=float, default=10, help='Weight negative loss for sage losss.')\n",
    "\n",
    "params,unknown=parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d514fe0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.TensorDataset object at 0x0000021DABDB2F70>\n"
     ]
    }
   ],
   "source": [
    "def load_data(feat_file,edge_file,SEP,dist_aware=True,hvg_file=None):\n",
    "    feat=pd.read_csv(feat_file[0],header=0,index_col=0,sep=SEP)\n",
    "    edge=np.loadtxt(edge_file[0],dtype=str)\n",
    "    batch={}\n",
    "    for i in np.arange(feat.shape[0]):\n",
    "        batch[i]=np.arange(feat.shape[0])\n",
    "    if len(edge.shape)<2:\n",
    "        edge=np.array([[i.split(\":\")[0],i.split(\":\")[1]] for i in edge])\n",
    "    if len(feat_file)>1:\n",
    "        for k in np.arange(1,len(feat_file)):\n",
    "            a=pd.read_csv(feat_file[k],header=0,index_col=0,sep=SEP)\n",
    "            for i in np.arange(feat.shape[0],a.shape[0]+feat.shape[0]):\n",
    "                batch[i]= np.arange(feat.shape[0],a.shape[0]+feat.shape[0])\n",
    "            if hvg_file is not None:\n",
    "                a=a.loc[:,np.loadtxt(hvg_file,dtype=str)]\n",
    "            feat=pd.concat((feat,a),axis=0)\n",
    "            a=np.loadtxt(edge_file[k],dtype=str)\n",
    "            if len(a.shape)<2:\n",
    "                a=np.array([[i.split(\":\")[0],i.split(\":\")[1]] for i in a])\n",
    "            edge=np.vstack((edge,a))\n",
    "    node_mapping=[{j:i for(i,j) in enumerate(feat.index)}]\n",
    "    node_mapping=node_mapping[0]\n",
    "    edge=np.array([[node_mapping[i[0]],node_mapping[i[1]]] for i in edge])\n",
    "    adj=process_adj(edge,feat.shape[0])\n",
    "#     pos_pairs=generate_pos_pair(adj.todense(),feat.index,N_WALKS,WALK_LEN)\n",
    "#     pos_pairs=np.array([[node_mapping[i[0]],node_mapping[i[1]]] for i in pos_pairs])\n",
    "    nodes=np.arange(feat.shape[0])\n",
    "#     neg_pairs=np.array(get_negtive_nodes(nodes,dist,adj,N_WALK_LEN,NUM_NEG,dist_aware))\n",
    "#     dataset=TensorDataset(torch.Tensor(list(node_mapping.values())).int(),torch.cat((torch.zeros(num1),torch.ones(num2))).int())\n",
    "    dataset=TensorDataset(torch.Tensor(list(nodes)).int())\n",
    "    return dataset,feat,adj,batch\n",
    "\n",
    "dataset,feat,adj,batch = load_data(feat_file,edge_file,SEP)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeebc174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 11023, 11024, 11025])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d75a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(spiral.main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "152bce16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.TensorDataset object at 0x000001C830D28DC0>\n",
      "[0. 0. 0. ... 5. 5. 5.]\n",
      "  (0, 10)\t1.0\n",
      "  (0, 98)\t1.0\n",
      "  (0, 274)\t1.0\n",
      "  (0, 285)\t1.0\n",
      "  (0, 425)\t1.0\n",
      "  (0, 678)\t1.0\n",
      "  (0, 680)\t1.0\n",
      "  (0, 685)\t1.0\n",
      "  (1, 2)\t1.0\n",
      "  (1, 14)\t1.0\n",
      "  (1, 15)\t1.0\n",
      "  (1, 414)\t1.0\n",
      "  (1, 422)\t1.0\n",
      "  (1, 429)\t1.0\n",
      "  (2, 1)\t1.0\n",
      "  (2, 14)\t1.0\n",
      "  (2, 414)\t1.0\n",
      "  (2, 420)\t1.0\n",
      "  (2, 421)\t1.0\n",
      "  (2, 429)\t1.0\n",
      "  (3, 13)\t1.0\n",
      "  (3, 25)\t1.0\n",
      "  (3, 38)\t1.0\n",
      "  (3, 419)\t1.0\n",
      "  (3, 426)\t1.0\n",
      "  :\t:\n",
      "  (57532, 57528)\t1.0\n",
      "  (57532, 57530)\t1.0\n",
      "  (57533, 57352)\t1.0\n",
      "  (57533, 57356)\t1.0\n",
      "  (57533, 57375)\t1.0\n",
      "  (57533, 57506)\t1.0\n",
      "  (57533, 57508)\t1.0\n",
      "  (57533, 57514)\t1.0\n",
      "  (57533, 57520)\t1.0\n",
      "  (57533, 57535)\t1.0\n",
      "  (57534, 57349)\t1.0\n",
      "  (57534, 57351)\t1.0\n",
      "  (57534, 57353)\t1.0\n",
      "  (57534, 57369)\t1.0\n",
      "  (57534, 57525)\t1.0\n",
      "  (57534, 57527)\t1.0\n",
      "  (57534, 57528)\t1.0\n",
      "  (57535, 57347)\t1.0\n",
      "  (57535, 57352)\t1.0\n",
      "  (57535, 57361)\t1.0\n",
      "  (57535, 57372)\t1.0\n",
      "  (57535, 57508)\t1.0\n",
      "  (57535, 57514)\t1.0\n",
      "  (57535, 57520)\t1.0\n",
      "  (57535, 57533)\t1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 15444, 14836, 9512, 17772, 16448, 2524, 10060) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mD:\\anaconda\\envs\\deepst_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1120\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\deepst_env\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m SPII\u001b[38;5;241m=\u001b[39mSPIRAL_integration(params,feat_file,edge_file,meta_file)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mSPII\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\bio\\SPIRAL\\SPIRAL-main\\spiral\\main.py:88\u001b[0m, in \u001b[0;36mSPIRAL_integration.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m t\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     87\u001b[0m IDX\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (batch_idx, target_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_loader):\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(np\u001b[38;5;241m.\u001b[39marray(IDX)))\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\deepst_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\deepst_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1316\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\deepst_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1282\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1282\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1283\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1284\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\deepst_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1132\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pids_str)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 15444, 14836, 9512, 17772, 16448, 2524, 10060) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "SPII=SPIRAL_integration(params,feat_file,edge_file,meta_file)\n",
    "SPII.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepst",
   "language": "python",
   "name": "deepst_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
