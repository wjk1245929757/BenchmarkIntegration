{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "927cbfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding : utf-8-*-\n",
    "# coding:unicode_escape\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# the location of R (used for the mclust clustering)\n",
    "import os\n",
    "os.environ['R_HOME'] = \"D:\\\\anaconda\\envs\\STAligner\\Lib\\R\"\n",
    "os.environ['R_USER'] = \"D:\\\\anaconda\\envs\\STAligner\\Lib\\site-packages\\rpy2\"\n",
    "\n",
    "\n",
    "os.chdir(\"D:/bio/SPIRAL/SPIRAL-main\") \n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from spiral.main import SPIRAL_integration\n",
    "from spiral.layers import *\n",
    "from spiral.utils import *\n",
    "from spiral.CoordAlignment import CoordAlignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6486446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs=\"G:/dataset/2_saggital/input/SPIRAL_input/\"\n",
    "sample_name=['anterior1', 'posterior1']\n",
    "for sample in sample_name:\n",
    "    df = pd.read_csv(dirs+sample+'_meta.csv', index_col=0)\n",
    "    df2 = df[['batch', 'celltype_mapped_refined']]\n",
    "    df2.to_csv(dirs+sample+'_label.csv')\n",
    "    df3 = df[['x_global', 'y_global']]\n",
    "    df3.to_csv(dirs+sample+'_coord.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b07964c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirs=\"G:/dataset/07-seqFish-SpatialMouseAtlas/input/SPIRAL_input/\"\n",
    "# sample_name=['embryo1-2', 'embryo1-5', 'embryo2-2','embryo2-5', 'embryo3-2', 'embryo3-5']\n",
    "dirs=\"G:/dataset/2_saggital/input/SPIRAL_input/\"\n",
    "sample_name=['anterior1', 'posterior1']\n",
    "samples=sample_name\n",
    "SEP=','\n",
    "net_cate='_KNN_'\n",
    "rad=150\n",
    "knn=6\n",
    "\n",
    "N_WALKS=knn\n",
    "WALK_LEN=1\n",
    "N_WALK_LEN=knn\n",
    "NUM_NEG=knn\n",
    "\n",
    "feat_file=[]\n",
    "edge_file=[]\n",
    "meta_file=[]\n",
    "coord_file=[]\n",
    "flags=''\n",
    "flags1=str(samples[0])\n",
    "for i in range(1,len(samples)):\n",
    "    flags1=flags1+'-'+str(samples[i])\n",
    "for i in range(len(samples)):\n",
    "    feat_file.append(dirs+samples[i]+\"_features.csv\")\n",
    "    edge_file.append(dirs+samples[i]+\"_edge_KNN_\"+str(knn)+\".csv\")\n",
    "    meta_file.append(dirs+samples[i]+\"_label.csv\")\n",
    "    coord_file.append(dirs+samples[i]+\"_coord.csv\")\n",
    "    flags=flags+'_'+str(samples[i])\n",
    "                     \n",
    "N=pd.read_csv(feat_file[0],header=0,index_col=0).shape[1]\n",
    "if (len(samples)==2):\n",
    "    M=1\n",
    "else:\n",
    "    M=len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a9ce85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=0, help='The seed of initialization.')\n",
    "parser.add_argument('--AEdims', type=list, default=[N,[512],32], help='Dim of encoder.')\n",
    "parser.add_argument('--AEdimsR', type=list, default=[32,[512],N], help='Dim of decoder.')\n",
    "parser.add_argument('--GSdims', type=list, default=[512,32], help='Dim of GraphSAGE.')\n",
    "parser.add_argument('--zdim', type=int, default=32, help='Dim of embedding.')\n",
    "parser.add_argument('--znoise_dim', type=int, default=4, help='Dim of noise embedding.')\n",
    "parser.add_argument('--CLdims', type=list, default=[4,[],M], help='Dim of classifier.')\n",
    "parser.add_argument('--DIdims', type=list, default=[28,[32,16],M], help='Dim of discriminator.')\n",
    "parser.add_argument('--beta', type=float, default=1.0, help='weight of GraphSAGE.')\n",
    "parser.add_argument('--agg_class', type=str, default=MeanAggregator, help='Function of aggregator.')\n",
    "parser.add_argument('--num_samples', type=str, default=knn, help='number of neighbors to sample.')\n",
    "\n",
    "parser.add_argument('--N_WALKS', type=int, default=N_WALKS, help='number of walks of random work for postive pairs.')\n",
    "parser.add_argument('--WALK_LEN', type=int, default=WALK_LEN, help='walk length of random work for postive pairs.')\n",
    "parser.add_argument('--N_WALK_LEN', type=int, default=N_WALK_LEN, help='number of walks of random work for negative pairs.')\n",
    "parser.add_argument('--NUM_NEG', type=int, default=NUM_NEG, help='number of negative pairs.')\n",
    "\n",
    "\n",
    "parser.add_argument('--epochs', type=int, default=100, help='Number of epochs to train.')\n",
    "parser.add_argument('--batch_size', type=int, default=1024, help='Size of batches to train.') ####512 for withon donor;1024 for across donor###\n",
    "parser.add_argument('--lr', type=float, default=1e-3, help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay.')\n",
    "parser.add_argument('--alpha1', type=float, default=N, help='Weight of decoder loss.')\n",
    "parser.add_argument('--alpha2', type=float, default=1, help='Weight of GraphSAGE loss.')\n",
    "parser.add_argument('--alpha3', type=float, default=1, help='Weight of classifier loss.')\n",
    "parser.add_argument('--alpha4', type=float, default=1, help='Weight of discriminator loss.')\n",
    "parser.add_argument('--lamda', type=float, default=1, help='Weight of GRL.')\n",
    "parser.add_argument('--Q', type=float, default=10, help='Weight negative loss for sage losss.')\n",
    "\n",
    "params,unknown=parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a662f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(feat_file,edge_file,SEP,dist_aware=True,hvg_file=None):\n",
    "    feat=pd.read_csv(feat_file[0],header=0,index_col=0,sep=SEP)\n",
    "    edge=np.loadtxt(edge_file[0],dtype=str)\n",
    "    batch={}\n",
    "    for i in np.arange(feat.shape[0]):\n",
    "        batch[i]=np.arange(feat.shape[0])\n",
    "    if len(edge.shape)<2:\n",
    "        edge=np.array([[i.split(\":\")[0],i.split(\":\")[1]] for i in edge])\n",
    "    if len(feat_file)>1:\n",
    "        for k in np.arange(1,len(feat_file)):\n",
    "            a=pd.read_csv(feat_file[k],header=0,index_col=0,sep=SEP)\n",
    "            for i in np.arange(feat.shape[0],a.shape[0]+feat.shape[0]):\n",
    "                batch[i]= np.arange(feat.shape[0],a.shape[0]+feat.shape[0])\n",
    "            if hvg_file is not None:\n",
    "                a=a.loc[:,np.loadtxt(hvg_file,dtype=str)]\n",
    "            feat=pd.concat((feat,a),axis=0)\n",
    "            a=np.loadtxt(edge_file[k],dtype=str)\n",
    "            if len(a.shape)<2:\n",
    "                a=np.array([[i.split(\":\")[0],i.split(\":\")[1]] for i in a])\n",
    "            edge=np.vstack((edge,a))\n",
    "    node_mapping=[{j:i for(i,j) in enumerate(feat.index)}]\n",
    "    node_mapping=node_mapping[0]\n",
    "    edge=np.array([[node_mapping[i[0]],node_mapping[i[1]]] for i in edge])\n",
    "    adj=process_adj(edge,feat.shape[0])\n",
    "#     pos_pairs=generate_pos_pair(adj.todense(),feat.index,N_WALKS,WALK_LEN)\n",
    "#     pos_pairs=np.array([[node_mapping[i[0]],node_mapping[i[1]]] for i in pos_pairs])\n",
    "    nodes=np.arange(feat.shape[0])\n",
    "#     neg_pairs=np.array(get_negtive_nodes(nodes,dist,adj,N_WALK_LEN,NUM_NEG,dist_aware))\n",
    "#     dataset=TensorDataset(torch.Tensor(list(node_mapping.values())).int(),torch.cat((torch.zeros(num1),torch.ones(num2))).int())\n",
    "    dataset=TensorDataset(torch.Tensor(list(nodes)).int())\n",
    "    return dataset,feat,adj,batch\n",
    "\n",
    "dataset,feat,adj,batch = load_data(feat_file,edge_file,SEP)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c487d65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 11023, 11024, 11025])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99c76e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "del sys.modules['spiral.main']\n",
    "from spiral.main import SPIRAL_integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45575a3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Training.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.24 GiB (GPU 0; 4.00 GiB total capacity; 3.37 GiB already allocated; 0 bytes free; 3.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m SPII\u001b[38;5;241m=\u001b[39mSPIRAL_integration(params,feat_file,edge_file,meta_file)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mSPII\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\bio\\SPIRAL\\SPIRAL-main\\spiral\\main.py:92\u001b[0m, in \u001b[0;36mSPIRAL_integration.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     90\u001b[0m all_rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madj\u001b[38;5;241m.\u001b[39mtolil()\u001b[38;5;241m.\u001b[39mrows[all_layer[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m     91\u001b[0m all_feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeat1[all_layer[\u001b[38;5;241m0\u001b[39m],:]\n\u001b[1;32m---> 92\u001b[0m all_embed,ae_out,clas_out,disc_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43mall_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mall_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43mall_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlamda\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mde_act\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcl_act\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m [ae_embed,gs_embed,embed]\u001b[38;5;241m=\u001b[39mall_embed\n\u001b[0;32m     94\u001b[0m [x_bar,x]\u001b[38;5;241m=\u001b[39mae_out\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\deepst_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\bio\\SPIRAL\\SPIRAL-main\\spiral\\model.py:270\u001b[0m, in \u001b[0;36mA_G_Combination_DA.forward\u001b[1;34m(self, x, node_layers, mappings, rows, lamda, de_act, cl_act)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,node_layers, mappings,rows,lamda,de_act,cl_act):\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrevgrad\u001b[38;5;241m=\u001b[39mRevGrad(lamda)\n\u001b[1;32m--> 270\u001b[0m     final_z,x_bar,x1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnode_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappings\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43mde_act\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m     ae_z,gs_z,z\u001b[38;5;241m=\u001b[39mfinal_z\n\u001b[0;32m    272\u001b[0m     znoise\u001b[38;5;241m=\u001b[39mz[:,:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mznoise_dim]\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\deepst_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\bio\\SPIRAL\\SPIRAL-main\\spiral\\model.py:252\u001b[0m, in \u001b[0;36mA_G_Combination.forward\u001b[1;34m(self, x, node_layers, mappings, rows, de_act)\u001b[0m\n\u001b[0;32m    250\u001b[0m         x1\u001b[38;5;241m=\u001b[39mx[[mappings[\u001b[38;5;241m0\u001b[39m][i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m node_layers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]],:]\n\u001b[0;32m    251\u001b[0m         _,ae_z\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mae\u001b[38;5;241m.\u001b[39men(x1)\n\u001b[1;32m--> 252\u001b[0m         gs_z\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnode_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappings\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m#         z=self.combine_layer(torch.cat((ae_z,gs_z),dim=1))\u001b[39;00m\n\u001b[0;32m    254\u001b[0m         z\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta)\u001b[38;5;241m*\u001b[39mae_z\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m*\u001b[39mgs_z\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\deepst_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\bio\\SPIRAL\\SPIRAL-main\\spiral\\model.py:229\u001b[0m, in \u001b[0;36mGraphSAGE.forward\u001b[1;34m(self, features, node_layers, mappings, rows)\u001b[0m\n\u001b[0;32m    227\u001b[0m aggregate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregators[k](out, nodes, mapping, cur_rows,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples)\u001b[38;5;241m.\u001b[39mcuda()                                        \n\u001b[0;32m    228\u001b[0m cur_mapped_nodes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([mapping[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m nodes], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m--> 229\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcur_mapped_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfcs[k](out)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.24 GiB (GPU 0; 4.00 GiB total capacity; 3.37 GiB already allocated; 0 bytes free; 3.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "SPII=SPIRAL_integration(params,feat_file,edge_file,meta_file)\n",
    "SPII.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepst",
   "language": "python",
   "name": "deepst_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
